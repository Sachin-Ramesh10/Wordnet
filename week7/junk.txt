from sklearn.mixture import GMM
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

for n in range(2,6):
    clusterer = GMM(n_components=n).fit(reduced_data)

            # TODO: Predict the cluster for each data point
    preds = clusterer.predict(reduced_data)

            # TODO: Find the cluster centers
    centers = clusterer.means_

            # TODO: Predict the cluster for each transformed sample data point
    reduced_samples = pd.DataFrame(pca_samples, columns = ['Dimension 1', 'Dimension 2'])
    sample_preds = clusterer.predict(reduced_samples)
            # TODO: Calculate the mean silhouette coefficient for the number of clusters chosen
    score = silhouette_score(reduced_data,preds)

    print score
'''    
0.367893070827
0.26621702439
0.307114882479
0.311327245752
'''


from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# TODO: Apply your clustering algorithm of choice to the reduced data 
clusterer = KMeans(n_clusters=5, random_state=25).fit(reduced_data)

# TODO: Predict the cluster for each data point
preds = clusterer.predict(reduced_data)

# TODO: Find the cluster centers
centers = clusterer.cluster_centers_

# TODO: Predict the cluster for each transformed sample data point
reduced_samples = pd.DataFrame(pca_samples, columns = ['Dimension 1', 'Dimension 2'])
sample_preds = clusterer.predict(reduced_samples)

# TODO: Calculate the mean silhouette coefficient for the number of clusters chosen
score= silhouette_score(reduced_data, clusterer.labels_, metric='euclidean')
print("Score for cluster of given size: ",score)

('Score for cluster of given size: ', 0.36834009201000828)
('Score for cluster of given size: ', 0.32558878934186453)
('Score for cluster of given size: ', 0.33339377388885649)
('Score for cluster of given size: ', 0.33446085563187711)




















import matplotlib.pyplot as plt
import matplotlib.cm as cm
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore", category = UserWarning, module = "matplotlib")
from IPython import get_ipython
get_ipython().run_line_magic('matplotlib', 'inline')



def biplot(good_data, reduced_data, pca):
    '''
    Produce a biplot that shows a scatterplot of the reduced
    data and the projections of the original features.
    
    good_data: original data, before transformation.
               Needs to be a pandas dataframe with valid column names
    reduced_data: the reduced data (the first two dimensions are plotted)
    pca: pca object that contains the components_ attribute
    return: a matplotlib AxesSubplot object (for any additional customization)
    
    This procedure is inspired by the script:
    https://github.com/teddyroland/python-biplot
    '''

    fig, ax = plt.subplots(figsize = (20,14))
    # scatterplot of the reduced data    
    ax.scatter(x=reduced_data.loc[:, 'Dimension 1'], y=reduced_data.loc[:, 'Dimension 2'], 
        facecolors='b', edgecolors='b', s=70, alpha=0.5)
    
    feature_vectors = pca.components_.T

    # we use scaling factors to make the arrows easier to see
    arrow_size, text_pos = 80.0, 100.0,

    # projections of the original features
    for i, v in enumerate(feature_vectors):
        ax.arrow(0, 0, arrow_size*v[0], arrow_size*v[1], 
                  head_width=0.2, head_length=0.2, linewidth=2, color='red')
        ax.text(v[0]*text_pos, v[1]*text_pos, good_data.columns[i], color='black', 
                 ha='center', va='center', fontsize=18)

    ax.set_xlabel("Dimension 1", fontsize=14)
    ax.set_ylabel("Dimension 2", fontsize=14)
    ax.set_title("PC plane with original feature projections.", fontsize=16);
    return ax

biplot(data, reduced_data, pca)






from sklearn.grid_search import GridSearchCV
def predict(d, y,n):
    
    #model1 = svm.SVC(kernel='linear', probability=True)
    model = LogisticRegression(penalty='l2', solver='newton-cg',multi_class = 'multinomial')
    #M = [model1,model2]
    #for i in M:
    X_train, X_test, y_train, y_test = train_test_split(d, y, test_size=.25)
    #model = OneVsRestClassifier(i)
    best_clf = grid_fit.best_estimator_
    y_score = model.fit(X_train, y_train).decision_function(X_test)
    predictions = (clf.fit(X_train, y_train)).predict(X_test)
    best_predictions = best_clf.predict(X_test)
    #predict = model.fit(X_train, y_train).predict(X_test)
    error(predict,y_test)
        
        #if(i == model1):
         #   curve(y_score,y_test,"SVM",n)
            
    curve(y_score,y_test,"logreg",n)